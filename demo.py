import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import BitsAndBytesConfig
from datasets import load_dataset
from tqdm import tqdm
import os
import logging

# ================= âš™ï¸ ç”¨æˆ·é…ç½®åŒºåŸŸ =================

# 1. æ¨¡å‹ ID
MODEL_ID = "/home/lhd/Qwen/Qwen3-30B-A3B-Instruct-2507"

# 2. åˆ†ææ¨¡å¼å¼€å…³
#   - è®¾ä¸ºæ•´æ•° (ä¾‹å¦‚ 10): åªåˆ†æç¬¬ 10 å±‚ (é€Ÿåº¦æœ€å¿«ï¼Œé€‚åˆè°ƒè¯•)
#   - è®¾ä¸º None: åˆ†ææ‰€æœ‰ MoE å±‚ (é€‚åˆå…¨é‡åˆ†æï¼Œç”Ÿæˆæ¯ä¸€å±‚çš„çƒ­åŠ›å›¾)
TARGET_LAYER = 30
ENABLE_COUNT_FILTER = True  # å¼€å…³ï¼šTrue è¡¨ç¤ºå¼€å¯é™åˆ¶ï¼ŒFalse è¡¨ç¤ºä¸é™åˆ¶
MIN_COUNT_THRESHOLD = 50    # é˜ˆå€¼ï¼šé™åˆ¶çš„æœ€å°æ¬¡æ•°

# 3. æ•°æ®é…ç½®
OUTPUT_DIR = "moe_analysis_report"
NUM_SAMPLES = 100        # é‡‡æ ·æ•°é‡ (æ ·æœ¬è¶Šå¤šè¶Šå‡†)
MAX_SEQ_LEN = 1024       # åºåˆ—é•¿åº¦
BATCH_SIZE = 4           # é€‚å½“å¢å¤§ Batch å¯åŠ é€Ÿæ¨ç†
NUM_COACTIVATORS = 10    # æ¯ä¸ªä¸“å®¶ top co-activators æ•°é‡
NUM_TOP_ACTIVE = 20      # æ‰“å°çƒ­é—¨ä¸“å®¶æ•°é‡  

# ===================================================

# è®¾ç½®æ—¥å¿—å’Œç›®å½•
os.makedirs(OUTPUT_DIR, exist_ok=True)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MoEContextAnalyzer:
    def __init__(self, model_id, output_dir):
        self.output_dir = output_dir
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # 1. åŠ è½½æ¨¡å‹
        logger.info(f"ğŸš€ Loading model: {model_id}...")
        try:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True
            )
            self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_id,
                quantization_config=bnb_config,
                torch_dtype="auto",
                device_map="auto",
                trust_remote_code=True
            ).eval()
        except Exception as e:
            logger.error(f"Failed to load model: {e}")
            exit(1)

        # 2. æ¢æµ‹ä¸“å®¶é…ç½®
        self._detect_config()
        
        # 3. åˆå§‹åŒ–ç»Ÿè®¡å­˜å‚¨å®¹å™¨
        # layer_stats[layer_idx] = { "co_matrix": Tensor, "counts": Tensor }
        self.layer_stats = {}
        self.hooks = []

    def _detect_config(self):
        """è‡ªåŠ¨è¯†åˆ«ä¸åŒæ¶æ„çš„ MoE é…ç½®"""
        config = self.model.config
        if hasattr(config, "num_experts"):
            self.num_experts = config.num_experts
            self.top_k = config.num_experts_per_tok
        elif hasattr(config, "num_local_experts"): # Mixtral / Llama-MoE
            self.num_experts = config.num_local_experts
            self.top_k = config.num_experts_per_tok
        else:
            logger.warning("âš ï¸ Unknown MoE config. Defaulting to 64 experts, top-k=2.")
            self.num_experts = 64
            self.top_k = 2
        
        logger.info(f"âš™ï¸ Config Detected: {self.num_experts} Experts, Top-{self.top_k} Routing")

    def _get_hook_fn(self, layer_idx):
        """ç”Ÿæˆé«˜æ•ˆçš„ Hook å‡½æ•°"""
        def hook_fn(module, input, output):
            # output shape: [batch, seq_len, num_experts] (logits)
            # å±•å¹³ -> [total_tokens, num_experts]
            logits = output.view(-1, output.size(-1))
            
            # æå– Top-K ç´¢å¼•
            # åªå– indicesï¼Œä¸éœ€è¦ gradients
            with torch.no_grad():
                _, indices = torch.topk(logits, k=self.top_k, dim=-1)
                
                # === âš¡ï¸ æ€§èƒ½å…³é”®ç‚¹ï¼šç«‹å³ç§»è‡³ CPU è®¡ç®— ===
                indices = indices.cpu()
                num_tokens = indices.shape[0]

                # åˆå§‹åŒ–è¯¥å±‚ç»Ÿè®¡å™¨ (å¦‚æœå°šæœªåˆå§‹åŒ–)
                if layer_idx not in self.layer_stats:
                    self.layer_stats[layer_idx] = {
                        "co_matrix": torch.zeros((self.num_experts, self.num_experts), dtype=torch.float64),
                        "counts": torch.zeros(self.num_experts, dtype=torch.float64)
                    }
                
                stats = self.layer_stats[layer_idx]

                # === âš¡ï¸ å‘é‡åŒ–è®¡ç®— (Vectorized) ===
                # æ„é€  Multi-hot Mask [tokens, experts]
                mask = torch.zeros(num_tokens, self.num_experts, dtype=torch.float64)
                mask.scatter_(1, indices.to(torch.long), 1.0)
                
                # 1. æ›´æ–°å•ä¸ªä¸“å®¶è®¡æ•° (Sum columns)
                stats["counts"] += mask.sum(dim=0)
                
                # 2. æ›´æ–°å…±ç°çŸ©é˜µ (Matrix Multiply)
                # Mask.T [E, N] @ Mask [N, E] = [E, E]
                stats["co_matrix"] += torch.matmul(mask.t(), mask)
                
        return hook_fn

    def register_hooks(self, target_layer=None):
        """
        target_layer: None (æ‰€æœ‰å±‚) æˆ– int (æŒ‡å®šå±‚)
        """
        logger.info("ğŸ”— Registering hooks...")
        count = 0
        
        # éå†æ¨¡å‹å±‚
        if hasattr(self.model, "model") and hasattr(self.model.model, "layers"):
            layers = self.model.model.layers
        else:
            logger.error("Could not find layers in model.")
            return

        for i, layer in enumerate(layers):
            # å¦‚æœæŒ‡å®šäº†å±‚ï¼Œä¸”å½“å‰å±‚ä¸æ˜¯ç›®æ ‡å±‚ï¼Œåˆ™è·³è¿‡
            if target_layer is not None and i != target_layer:
                continue
            
            # å¯»æ‰¾ Gate æ¨¡å—
            target_module = None
            if hasattr(layer, "mlp") and hasattr(layer.mlp, "gate"): # Qwen
                target_module = layer.mlp.gate
            elif hasattr(layer, "block_sparse_moe") and hasattr(layer.block_sparse_moe, "gate"): # Mixtral
                target_module = layer.block_sparse_moe.gate
            
            if target_module:
                h = target_module.register_forward_hook(self._get_hook_fn(i))
                self.hooks.append(h)
                count += 1
        
        if count == 0:
            logger.error(f"âŒ No MoE layers hooked! (Target: {target_layer})")
        else:
            logger.info(f"âœ… Hooked {count} layers.")

    def run_inference(self, num_samples, batch_size, seq_len):
        """åŠ è½½æ•°æ®å¹¶è¿è¡Œæ¨ç†"""
        logger.info("ğŸ“š Preparing Dataset (WikiText)...")
        try:
            ds = load_dataset("wikitext", "wikitext-2-raw-v1", split="test")
            # è¿‡æ»¤çŸ­æ–‡æœ¬
            raw_texts = [x['text'] for x in ds if len(x['text']) > 200]
            texts = raw_texts[:num_samples]
        except Exception as e:
            logger.warning(f"Dataset load failed ({e}), using dummy data.")
            texts = ["AI scaling is fascinating. " * 50] * num_samples

        logger.info(f"ğŸƒ Running Inference on {len(texts)} samples...")
        
        with torch.no_grad():
            for i in tqdm(range(0, len(texts), batch_size), desc="Inferencing"):
                batch = texts[i : i + batch_size]
                inputs = self.tokenizer(
                    batch, 
                    return_tensors="pt", 
                    padding=True, 
                    truncation=True, 
                    max_length=seq_len
                ).to(self.device)
                
                self.model(**inputs)
        
        # æ¸…ç† Hooks
        for h in self.hooks: h.remove()


    def generate_visualizations(self):
        """åˆ†æç»“æœå¹¶ç»˜å›¾ï¼ŒåŒæ—¶ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š"""
        logger.info("ğŸ“Š Generating Heatmaps & Report...")
        
        # åˆ›å»ºæ–‡æœ¬æŠ¥å‘Šè·¯å¾„
        report_path = os.path.join(self.output_dir, "analysis_summary.txt")
        
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(f"MoE Analysis Report\nModel: {MODEL_ID}\n{'='*40}\n\n")
            
            for layer_idx, stats in self.layer_stats.items():
                co_matrix = stats["co_matrix"]
                counts = stats["counts"]
                
                # è®¡ç®—æ¡ä»¶æ¦‚ç‡çŸ©é˜µ P(j | i)
                # é¿å…é™¤ä»¥ 0
                safe_counts = counts.clone()
                safe_counts[safe_counts == 0] = 1.0
                
                # å¹¿æ’­é™¤æ³• [E, E] / [E, 1]
                prob_matrix = co_matrix / safe_counts.unsqueeze(1)
                prob_np = prob_matrix.numpy()
                
                # === å¤„ç†å¯¹è§’çº¿ (è®¾ä¸º NaN ä»¥ä¾¿ç»˜å›¾æ—¶ç•™ç™½) ===
                np.fill_diagonal(prob_np, np.nan)
                
                # === ç»˜å›¾ ===
                plt.figure(figsize=(10, 9))
                sns.set_context("notebook")
                
                # åªæœ‰å½“åªæœ‰ä¸€å±‚æ—¶ï¼Œæ‰æ˜¾ç¤ºå…·ä½“æ•°å€¼ï¼Œå¦åˆ™å…¨å±‚åˆ†ææ—¶å­—å¤ªå°
                annot = True if len(self.layer_stats) == 1 and self.num_experts < 32 else False
                
                sns.heatmap(
                    prob_np,
                    cmap="turbo",      # é«˜å¯¹æ¯”åº¦é¢œè‰²
                    square=True,
                    xticklabels=False, # ä¸“å®¶å¤ªå¤šä¸æ˜¾ç¤ºå…·ä½“ç¼–å·
                    yticklabels=False,
                    annot=annot,
                    fmt=".2f",
                    cbar_kws={'label': 'P(Expert J | Expert I)'}
                )
                
                mode_str = "Single Layer" if len(self.layer_stats) == 1 else "All Layers Scan"
                plt.title(f"Layer {layer_idx} Co-activation ({mode_str})\nModel: {MODEL_ID}")
                plt.xlabel("Expert J (Co-activated)")
                plt.ylabel("Expert I (Pivot)")
                
                filename = f"heatmap_layer_{layer_idx:02d}.png"
                save_path = os.path.join(self.output_dir, filename)
                plt.savefig(save_path, dpi=150)
                plt.close() # å…³é—­ç”»å¸ƒé‡Šæ”¾å†…å­˜
                
                # === æ‰“å° Top å…³è”å¹¶å†™å…¥æ–‡ä»¶ ===
                self._report_top_pairs(layer_idx, prob_np, counts, f)
            
        logger.info(f"âœ… Analysis Complete! Results saved to: {self.output_dir}")
        logger.info(f"ğŸ“„ Detailed Text Report: {report_path}")

    def _report_top_pairs(self, layer_idx, prob_np, counts, file_handle):
        total_activations = counts.sum().item()
        total_tokens = total_activations / self.top_k if self.top_k > 0 else 1.0
        
        # æŒ‰æ¿€æ´»æ¬¡æ•°é™åºæ’åˆ—
        sorted_indices = torch.argsort(counts, descending=True)
        
        header_active = f"\n--- Layer {layer_idx} Top {NUM_TOP_ACTIVE} Active Experts ---"
        print(header_active)
        file_handle.write(header_active + "\n")
        
        for i in range(min(NUM_TOP_ACTIVE, len(counts))):
            idx = sorted_indices[i].item()
            cnt = counts[idx].item()
            
            # === ä¿®æ”¹ï¼šè®¡ç®—ç›¸å¯¹äº Token æ€»æ•°çš„æ¦‚ç‡ ===
            ratio = cnt / total_tokens if total_tokens > 0 else 0
            
            # æ ¼å¼: Expert XX (Count/TotalTokens) Ratio
            line = f"Expert {idx:02d} ({int(cnt)}/{int(total_tokens)}) {ratio:.2%}"
            print(line)
            file_handle.write("  " + line + "\n")

        """åœ¨æ§åˆ¶å°æ‰“å° Top å…³è”å¯¹ï¼Œå¹¶å†™å…¥æ–‡ä»¶"""
        # å°†çŸ©é˜µå±•å¹³å¹¶æ’åºï¼Œæ‰¾åˆ°æ¦‚ç‡æœ€é«˜çš„ç´¢å¼•
        flat_indices = np.argsort(np.nan_to_num(prob_np).flatten())[::-1]
        
        header = f"\n--- Layer {layer_idx} Strongest Correlations ---"
        print(header)
        file_handle.write(header + "\n")
        
        # 1. å…¨å±€ Top Pairs (æ¦‚ç‡æœ€é«˜çš„å‡ å¯¹)
        file_handle.write(">>> Global Top Pairs (P(J|I)):\n")
        count_printed = 0
        for idx in flat_indices:
            r = idx // self.num_experts
            c = idx % self.num_experts
            val = prob_np[r, c]
            
            # è¿‡æ»¤ï¼šå¦‚æœæ˜¯ NaNï¼Œåˆ™å¿½ç•¥
            if np.isnan(val) or (ENABLE_COUNT_FILTER and counts[r] < MIN_COUNT_THRESHOLD):
                continue
            
            line = f"Exp {r:02d} -> Exp {c:02d} : {val:.1%} (Pivot Count: {int(counts[r])})"
            
            # æ§åˆ¶å°æ‰“å°å‰ 5 ä¸ªï¼Œè®©ä½ ä¸€çœ¼çœ‹åˆ°æœ€å¼ºçš„
            if count_printed < 5:
                print(line)
            
            # æ–‡ä»¶å†™å…¥å‰ 20 ä¸ª
            if count_printed < 20:
                file_handle.write("  " + line + "\n")
            
            if count_printed >= 20:
                break

            count_printed += 1
            
        # 2. æ¯ä¸ªä¸“å®¶çš„ Top Co-activators (æ›´è¯¦ç»†çš„åˆ—è¡¨)
        file_handle.write("\n>>> Top Co-activators per Expert:\n")
        for r in range(self.num_experts):
            if ENABLE_COUNT_FILTER and counts[r] < MIN_COUNT_THRESHOLD: continue

            # è·å–è¯¥ä¸“å®¶çš„è¡Œ
            row = np.nan_to_num(prob_np[r])
            # æ’åºæ‰¾åˆ° Top NUM_COACTIVATORS
            top_indices = np.argsort(row)[::-1][:NUM_COACTIVATORS]
            
            partners = []
            for c in top_indices:
                val = row[c]
                partners.append(f"Exp{c:02d}({val:.0%})")
            
            file_handle.write(f"  Expert {r:02d}: " + ", ".join(partners) + "\n")
        
        file_handle.write("\n")


# ================= ğŸš€ ä¸»ç¨‹åºå…¥å£ =================

if __name__ == "__main__":
    analyzer = MoEContextAnalyzer(MODEL_ID, OUTPUT_DIR)
    
    # æ ¹æ®é…ç½®æ³¨å†Œ Hook
    # å¦‚æœ TARGET_LAYER æ˜¯æ•°å­—ï¼Œåª Hook é‚£ä¸€å±‚
    # å¦‚æœ TARGET_LAYER æ˜¯ Noneï¼ŒHook æ‰€æœ‰å±‚
    analyzer.register_hooks(target_layer=TARGET_LAYER)
    
    # è¿è¡Œ
    analyzer.run_inference(NUM_SAMPLES, BATCH_SIZE, MAX_SEQ_LEN)
    analyzer.generate_visualizations()